# Introduction
This repository comprises notebooks covering the primary code stacks and skills required to progress through deep learning material. The repository targets a graduate-level audience and focuses on deep learning concepts. It assumes you have been exposed to traditional machine learning concepts, like supervised, unsupervised, dimensional reduction, and linear algebra. It also assumes you grasp frameworks such as scikit-learn, pytorch, and tensorflow. These tutorials will be primarily written in JAX, to provide the most interpretable detail between mathematical proofs and code implementations.

# Notebook Outline
 - Fundamental Concepts in Linear Algebra and Calculus through JAX
   - This notebook serves as a fundamental review of machine learning content as well as an introduction to JAX
 - Backpropagation example w/ the Delta rule and LMS
   - This is a review of an adaptive filtering implementation with JAX
 - Multi-layer perceptrons w/ JAX
   - MLP w/ JAX
 - Convolutional Neural Networks from Scratch w/ JAX
   - CNNs with JAX
 - Transformers w/JAX
   - Transformer Implementation with JAX
